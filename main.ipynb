{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from flask import Flask\n",
    " \n",
    "# Flask utils\n",
    "from flask import render_template, request, jsonify\n",
    "from raptor import RetrievalAugmentation \n",
    "\n",
    "# Flask utils\n",
    "# from flask import Flask, redirect, url_for, request, render_template\n",
    "from werkzeug.utils import secure_filename\n",
    "# from gevent.pywsgi import WSGIServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a flask app\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 17:22:45,429 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x00000228E3E30A40>\n",
      "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x00000228E3E30AA0>}\n",
      "            Cluster Embedding Model: OpenAI\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-04-28 17:22:45,430 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x00000228E3E30A40>\n",
      "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x00000228E3E30AA0>}\n",
      "            Cluster Embedding Model: OpenAI\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-04-28 17:22:45,430 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x00000228E3E30A40>\n",
      "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x00000228E3E30AA0>}\n",
      "            Cluster Embedding Model: OpenAI\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: None\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: OpenAI\n",
      "            Embedding Model: <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x00000228E3D439B0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <raptor.QAModels.GPT3TurboQAModel object at 0x00000228E3E405F0>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "RA = RetrievalAugmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    # Main page\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct(text):\n",
    "    # construct the tree\n",
    "    RA.add_documents(text)\n",
    "    \n",
    "    print(\"Tree constructed\")\n",
    "    \n",
    "    SAVE_PATH = \"demo/mytree\"\n",
    "    RA.save(SAVE_PATH)\n",
    "    \n",
    "    print(\"Tree saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/process_query', methods=['POST'])\n",
    "def process_query():\n",
    "    question = request.json['data']\n",
    "    print(\"question: \", question)\n",
    "    \n",
    "    answer = RA.answer_question(question=question)\n",
    "    print(\"Answer: \", answer)\n",
    "    \n",
    "    # Do something with the data, for example, you can just echo it back\n",
    "    # generate_response(answer)\n",
    "    #return jsonify({'response': answer})\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        # Get the file from post request\n",
    "        f = request.files['file']\n",
    "\n",
    "        # Save the file to ./uploads\n",
    "        # Use os.path.abspath('') for .ipynb files\n",
    "        basepath = os.path.abspath('')\n",
    "\n",
    "        # Use os.path.dirname('') for .py files\n",
    "        # basepath = os.path.dirname(__file__)\n",
    "        \n",
    "        file_path = os.path.join(\n",
    "            basepath, 'uploads', secure_filename(f.filename))\n",
    "        f.save(file_path)\n",
    "        \n",
    "        print(file_path)\n",
    "    \n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        construct(text)\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 17:22:45,551 - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:4444\n",
      " * Running on http://192.168.0.10:4444\n",
      "2024-04-28 17:22:45,552 - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2024-04-28 17:22:47,470 - 192.168.0.10 - - [28/Apr/2024 17:22:47] \"GET / HTTP/1.1\" 200 -\n",
      "2024-04-28 17:22:47,494 - 192.168.0.10 - - [28/Apr/2024 17:22:47] \"\u001b[36mGET /static/css/main.css HTTP/1.1\u001b[0m\" 304 -\n",
      "2024-04-28 17:22:47,502 - 192.168.0.10 - - [28/Apr/2024 17:22:47] \"\u001b[36mGET /static/js/main.js HTTP/1.1\u001b[0m\" 304 -\n",
      "2024-04-28 17:22:54,694 - Splitting Text\n",
      "2024-04-28 17:22:54,698 - Creating Leaf Nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SLY\\Documents\\GitHub\\RAPTOR2\\uploads\\sample.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 17:22:55,158 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,163 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,181 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,187 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,190 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,194 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,201 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,209 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,210 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,211 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,213 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,218 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,223 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,264 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,272 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,275 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,465 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,475 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,518 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,523 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,525 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,546 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,559 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,570 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,574 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,579 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,580 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,585 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,588 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,594 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,601 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,604 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,797 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:22:55,803 - Created 33 Leaf Embeddings\n",
      "2024-04-28 17:22:55,803 - Building All Nodes\n",
      "2024-04-28 17:22:55,823 - Using Cluster TreeBuilder\n",
      "2024-04-28 17:22:55,824 - Constructing Layer 0\n",
      "2024-04-28 17:22:58,242 - Summarization Length: 100\n",
      "2024-04-28 17:23:00,620 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:00,625 - Node Texts Length: 608, Summarized Text Length: 100\n",
      "2024-04-28 17:23:00,972 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:04,081 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:04,084 - Node Texts Length: 534, Summarized Text Length: 100\n",
      "2024-04-28 17:23:04,425 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:06,459 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:06,463 - Node Texts Length: 482, Summarized Text Length: 100\n",
      "2024-04-28 17:23:06,794 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:11,908 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:11,911 - Node Texts Length: 530, Summarized Text Length: 100\n",
      "2024-04-28 17:23:12,362 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:14,875 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:14,877 - Node Texts Length: 175, Summarized Text Length: 100\n",
      "2024-04-28 17:23:15,180 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:15,183 - Constructing Layer 1\n",
      "2024-04-28 17:23:15,184 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 1\n",
      "2024-04-28 17:23:15,184 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: None\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: OpenAI\n",
      "            Embedding Model: <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x00000228E3D439B0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "2024-04-28 17:23:15,188 - Tree successfully saved to demo/mytree\n",
      "2024-04-28 17:23:15,189 - Exception on /upload [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SLY\\.conda\\envs\\raptor\\Lib\\site-packages\\flask\\app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SLY\\.conda\\envs\\raptor\\Lib\\site-packages\\flask\\app.py\", line 883, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SLY\\.conda\\envs\\raptor\\Lib\\site-packages\\flask\\app.py\", line 902, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SLY\\.conda\\envs\\raptor\\Lib\\site-packages\\flask\\app.py\", line 1174, in make_response\n",
      "    raise TypeError(\n",
      "TypeError: The view function for 'upload' did not return a valid response. The function either returned None or ended without a return statement.\n",
      "2024-04-28 17:23:15,190 - 192.168.0.10 - - [28/Apr/2024 17:23:15] \"\u001b[35m\u001b[1mPOST /upload HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree constructed\n",
      "Tree saved\n",
      "question:  How did Cinderella reach her happy ending?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 17:23:30,382 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:33,525 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-28 17:23:33,529 - 192.168.0.10 - - [28/Apr/2024 17:23:33] \"POST /process_query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Cinderella reached her happy ending by enduring mistreatment from her stepmother and stepsisters, remaining hopeful and dreaming of attending the royal festival. With the help of her fairy godmother, she was transformed into a vision of beauty and attended the event in a golden dress. At the festival, the king's son recognized her as the true bride he had danced with and chose her as his partner. Despite the deception of her stepsisters, Cinderella's true identity was revealed when the golden slipper fit her perfectly. The king's son married Cinderella, and she forgave her step-family, receiving a branch from the hazel bush as a gift. Cinderella planted the branch on her mother's grave, and it grew into a handsome tree, symbolizing her happy ending and new beginning.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host=os.getenv('IP', '0.0.0.0'), port=int(os.getenv('PORT', 4444)))\n",
    "    # app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPTOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
